{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tillschaland/Project/Gutemberg-Graph-Network/gutenbergCorpus\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/tillschaland/Project/Gutemberg-Graph-Network/GutenbergCorpus/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tillschaland/miniconda3/envs/gutenbergDBInit/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import  os\n",
    "import definitions\n",
    "import regex as re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from definitions import DATA_DIR\n",
    "all_books=os.listdir(DATA_DIR+'books/final/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs=[]\n",
    "\n",
    "for i,file in enumerate(all_books):\n",
    "    df = pd.read_json(f\"./data/books/final/{file}\",orient='index')\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCharacterInfo(df):\n",
    "    characterlist=list()\n",
    "    bookid=None\n",
    "    for i,row in df.iterrows():\n",
    "        characterlist.extend(list(row.PERS))\n",
    "        bookid=row.bookindex\n",
    "\n",
    "    characterlist=list(set(characterlist))\n",
    "    temp_=[]\n",
    "    for i,char in enumerate(characterlist):\n",
    "        persdict=dict()\n",
    "        persdict['sentenceBookID']=i\n",
    "        persdict['name']=char\n",
    "        persdict['bookID']=bookid\n",
    "        temp_.append(persdict)\n",
    "    return temp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBookFixture(dfs):\n",
    "    fixtures_=[] # 'bookindex','id', 'title','author','authoryearofbirth','authoryearofdeath','language','subjects'\n",
    "    for i,df in enumerate(dfs):\n",
    "        series=df.iloc[0]\n",
    "        fixtures_.append({'pk':series.bookindex,\n",
    "                          'id':series.id,\n",
    "                          'title':series.title,\n",
    "                          'author':series.author,\n",
    "                          'authoryearofbirth':series.authoryearofbirth,\n",
    "                          'authoryearofdeath':series.authoryearofdeath,\n",
    "                          'language':series.language,\n",
    "                          'subjects':series.subjects\n",
    "                          })\n",
    "        \n",
    "    return fixtures_\n",
    "\n",
    "def createSentFixture(df, bookFix, topicFix):\n",
    "    temp_=[]# sentenceid(pk), bookid, sentenceText,sentIDBook, topicID(pk)\n",
    "    for i,row in df.iterrows():\n",
    "        try:\n",
    "            topic=next(filter(lambda x: x['Topic']==row.Topic and x['Name']==row.Name,topicFix))\n",
    "        except  TypeError as err:\n",
    "            print(\"No Topic Found\")            \n",
    "        try:\n",
    "            book=next(filter(lambda x: x['id']==row.id,bookFix))\n",
    "        except TypeError as err:\n",
    "            print(\"Noo book found\")\n",
    "        temp_.append(\n",
    "            {\n",
    "            'pk':row.sentenceIndex,\n",
    "            'bookID': book['pk'],\n",
    "            'sentenceText':row.sents,\n",
    "            'sentIDBook':i,\n",
    "            'topicID':topic['pk'],\n",
    "            }\n",
    "        )\n",
    "    return temp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    df.rename({'index':'sentenceIndex'},axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractChar(df):\n",
    "    allChars = list()\n",
    "    for i,row in df.iterrows():\n",
    "        chars_ =row.PERS\n",
    "        for char in chars_:\n",
    "            allChars.append(char)\n",
    "    return list(set(allChars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTopics(df):\n",
    "    rel_cols=['bookindex','id','Topic','Name','Top_n_words']\n",
    "    temp_ = df[rel_cols]\n",
    "    listoftopics=list()\n",
    "    for i,row in temp_.iterrows():\n",
    "        dict_=dict()\n",
    "        dict_['Topic']=row.Topic\n",
    "        dict_['Name']=row.Name\n",
    "        dict_['Top_n_words']=processTopics(row.Top_n_words)\n",
    "        dict_['bookindex']=row.bookindex\n",
    "        dict_['id']=row.id\n",
    "        listoftopics.append(dict_)\n",
    "    return listoftopics\n",
    "\n",
    "def processTopics(topnwords):\n",
    "    list_of_words=topnwords.split('-')\n",
    "    list_of_words=[word.strip(' ') for word in list_of_words]\n",
    "    return list_of_words\n",
    "\n",
    "\n",
    "def removeDup(l):\n",
    "    new_d = []\n",
    "    for x in l:\n",
    "        if x not in new_d:\n",
    "            new_d.append(x)\n",
    "    return new_d\n",
    "\n",
    "def removeDictDuplicates(l):\n",
    "    seen = set()\n",
    "    new_l = []\n",
    "    for d in l:\n",
    "        t = tuple(d.items())\n",
    "        if t not in seen:\n",
    "            seen.add(t)\n",
    "            new_l.append(d)        \n",
    "    return sorted(new_l, key=lambda x: x['Topic'])   # sort by topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "characterTable=[]\n",
    "for df in dfs:\n",
    "    characterTable.extend(createCharacterInfo(df))\n",
    "\n",
    "characterTable =  [{'pk':i, 'name':char['name'],'bookID':char['bookID'], 'sentenceBookID':char['sentenceBookID']}for i,char in enumerate(characterTable)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=removeDup(extractTopics(df))\n",
    "t =  [{'pk':i, 'Topic':topic['Topic'],'Name':topic['Name'], 'top_n_word':topic['Top_n_words'],'bookindex':topic['bookindex']}for i,topic in enumerate(t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSent2Char(df, dfs_character):\n",
    "\n",
    "    def helpFilter(charDict, char):\n",
    "        return charDict['name']==char\n",
    "    \n",
    "    sent2char=[]\n",
    "\n",
    "    for i,row in df.iterrows():\n",
    "        characters=row.PERS\n",
    "        for char in characters:\n",
    "            filteredChar = next(filter(lambda x: helpFilter(x,char), dfs_character))\n",
    "            charpk = filteredChar['pk']\n",
    "            sentID = row.sentenceIndex\n",
    "            sent2char.append({'charID':charpk, 'sentID':sentID})\n",
    "\n",
    "    return sent2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isTopic(id,item):\n",
    "    \"\"\"\n",
    "    Filter Function for filter()\n",
    "    \n",
    "    \"\"\"\n",
    "    return id==item['Topic']   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_book=[] # 'bookindex','id', 'title','author','authoryearofbirth','authoryearofdeath','language','subjects'\n",
    "dfs_sentence=[] # sentenceid(pk), bookid, sentenceText,sentIDBook, topicID(pk)\n",
    "dfs_Sent2Char=[] # charID(sent.pk), sentID(sent.pk)\n",
    "dfs_character=[] # id(pk),name, bookid(book(pk))\n",
    "dfs_topic=[] # id(pk), bookid(book.pk), bookInternID(topicID),name, topnwords\n",
    "for df in dfs:\n",
    "    #df=df.reset_index().rename({'index':'sentIDBook'},axis=1)\n",
    "    dfs_book.append(df[['bookindex','id', 'title','author','authoryearofbirth','authoryearofdeath','language','subjects']])\n",
    "    dfs_sentence.append(df[['sentenceIndex', 'id','sents','Topic']])\n",
    "    dfs_character.extend(createCharacterInfo(df))\n",
    "    dfs_topic.extend(removeDup(extractTopics(df)))\n",
    "\n",
    "characterFixture =  [{'pk':i, 'name':char['name'],'bookID':char['bookID'], 'sentenceBookID':char['sentenceBookID']}for i,char in enumerate(dfs_character)]\n",
    "topicFixture =  [{'pk':i, 'Topic':topic['Topic'],'Name':topic['Name'], 'Top_n_words':topic['Top_n_words'],'bookindex':topic['bookindex']}for i,topic in enumerate(dfs_topic)]\n",
    "bookFixture=createBookFixture(dfs)\n",
    "sentenceFixture=[]\n",
    "sent2charFixture=[]\n",
    "for df in dfs:\n",
    "    sent2charFixture.extend(createSent2Char(df, characterFixture))\n",
    "    fitures=createSentFixture(df,bookFixture,topicFixture)\n",
    "    sentenceFixture.extend(fitures)\n",
    "\n",
    "sent2charFixture=[{'pk':i, 'charID':fix['charID'], 'sentID':fix['sentID']} for i,fix in enumerate(sent2charFixture)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFinalCharacterFixtur(fixture):\n",
    "    temp_=[]\n",
    "    for fix in fixture:\n",
    "        temp_.append(\n",
    "            {\n",
    "            \"model\":\"webDB.Character\",\n",
    "            \"pk\":fix['pk'],\n",
    "            \"fields\": {\n",
    "                'Name':fix['name'],\n",
    "                'bookID':fix['bookID'],\n",
    "            }\n",
    "            }\n",
    "        )\n",
    "    return temp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFinalTopicFixtur(fixture):\n",
    "    temp_=[]\n",
    "    for fix in fixture:\n",
    "        temp_.append(\n",
    "            {\n",
    "            \"model\":\"webDB.Topic\",\n",
    "            \"pk\":fix['pk'],\n",
    "            \"fields\": {\n",
    "                'TopicID':fix['Topic'],\n",
    "                'TopicName':fix['Name'],\n",
    "                'Top_n_words':fix['Top_n_words'],\n",
    "                'bookID':fix['bookindex']\n",
    "            }\n",
    "            }\n",
    "        )\n",
    "    return temp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFinalSent2CharFixtur(fixture):\n",
    "    temp_=[]\n",
    "    for fix in fixture:\n",
    "        temp_.append(\n",
    "            {\n",
    "            \"model\":\"webDB.Sent2Char\",\n",
    "            \"pk\":fix['pk'],\n",
    "            \"fields\": {\n",
    "                'charID':fix['charID'],\n",
    "                'sentID':fix['sentID']\n",
    "            }\n",
    "            }\n",
    "        )\n",
    "    return temp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFinalSentenceFixtur(fixture):\n",
    "    temp_=[]\n",
    "    for fix in fixture:\n",
    "        temp_.append(\n",
    "            {\n",
    "            \"model\":\"webDB.Sentence\",\n",
    "            \"pk\":int(fix['pk']),\n",
    "            \"fields\": {\n",
    "                'bookID':int(fix['bookID']),\n",
    "                'sentenceText':fix['sentenceText'],\n",
    "                'sentIDBook':fix['sentIDBook'],\n",
    "                'topicID':fix['topicID']\n",
    "            }\n",
    "            }\n",
    "        )\n",
    "    return temp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFinalBookFixtur(fixture):\n",
    "    temp_=[]\n",
    "    for fix in fixture:\n",
    "        temp_.append(\n",
    "            {\n",
    "            \"model\":\"webDB.Book\",\n",
    "            \"pk\":int(fix['pk']),\n",
    "            \"fields\": {\n",
    "                \"bookID\":fix['id'],\n",
    "                \"bookName\":fix['title'],\n",
    "                \"bookauthor\":fix['author'],\n",
    "                \"bookAuthorYearOfBirth\":str(fix['authoryearofbirth']),\n",
    "                \"bookAuthorYearOfDeath\":str(fix['authoryearofdeath']),\n",
    "                \"bookLanguage\":fix['language'],\n",
    "                \"bookSubjects\":fix['subjects'],\n",
    "            }\n",
    "            }\n",
    "        )\n",
    "    return temp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "characterFixture=createFinalCharacterFixtur(characterFixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicFixture=createFinalTopicFixtur(topicFixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookFixture=createFinalBookFixtur(bookFixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceFixture=createFinalSentenceFixtur(sentenceFixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2charFixture=createFinalSent2CharFixtur(sent2charFixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_int(x):\n",
    "    x = x['fields']['bookID']\n",
    "    print(x)\n",
    "    return int(x)\n",
    "sentenceFixture=[to_int(sent)for sent in sentenceFixture]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "fixtureFileNames=['character','topic','book','sentence','sent2char']\n",
    "fixturesCombined=[characterFixture,topicFixture,bookFixture,sentenceFixture,sent2charFixture]\n",
    "for fixture, filename in zip(fixturesCombined,fixtureFileNames):\n",
    "    with open(f\"./fixtures/Fixtures_Mid/{filename}FixturesSmall.json\", 'w',encoding='utf-8') as f:\n",
    "        json.dump(fixture,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'TopicName:': '0_kites_kite_kansas_mississippi', 'Count': 1}, 6: {'TopicName:': '6_1961_august_june_21', 'Count': 0}, 5: {'TopicName:': '5_trees_groves_cottonwoods_cottonwood', 'Count': 0}, -1: {'TopicName:': '-1_breeding_op_park_page', 'Count': 0}, 2: {'TopicName:': '2_adults_yearlings_difference_ratio', 'Count': 0}, 3: {'TopicName:': '3_food_adult_minutes_left', 'Count': 0}, 1: {'TopicName:': '1_nest_nests_fledglings_fly', 'Count': 0}, 7: {'TopicName:': '7_pellets_insects_total_205', 'Count': 0}, 4: {'TopicName:': '4_unspecified_prey_sp_insects', 'Count': 0}, 8: {'TopicName:': '8_lizards_snakes_small_predation', 'Count': 0}}\n"
     ]
    }
   ],
   "source": [
    "list_of_Sent=list(filter(lambda x: x['fields']['charID']==15, sent2charFixture))\n",
    "\n",
    "for sent in list_of_Sent:\n",
    "    toSearch=sent['fields']['sentID']\n",
    "    sentences=list(filter(lambda x: x['pk']==toSearch,sentenceFixture))\n",
    "\n",
    "bookid=next(filter(lambda x: x['pk']==15, characterFixture))['fields']['bookID']\n",
    "allTopics=list(filter(lambda x: x['fields']['bookID']==bookid, topicFixture))\n",
    "\n",
    "distr_=dict()\n",
    "for topic in allTopics:\n",
    "    distr_[topic['fields']['TopicID']]= {'TopicName:':topic['fields']['TopicName'], 'Count':0}\n",
    "for sent in sentences:\n",
    "    distr_[sent['fields']['topicID']]['Count']+=1\n",
    "\n",
    "\n",
    "print(distr_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gutenbergDBInit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
