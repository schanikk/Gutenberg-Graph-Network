{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tillschaland/Project/Gutemberg-Graph-Network/gutenbergCorpus\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/tillschaland/Project/Gutemberg-Graph-Network/GutenbergCorpus/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tillschaland/miniconda3/envs/gutenbergDBInit/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import  os\n",
    "import definitions\n",
    "import regex as re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from definitions import DATA_DIR\n",
    "all_books=os.listdir(DATA_DIR+'books/final/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs=[]\n",
    "\n",
    "for i,file in enumerate(all_books):\n",
    "    df = pd.read_json(f\"./data/books/final/{file}\",orient='index')\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCharacterInfo(df):\n",
    "    characterlist=list()\n",
    "    bookid=None\n",
    "    for i,row in df.iterrows():\n",
    "        characterlist.extend(list(row.PERS))\n",
    "        bookid=row.bookindex\n",
    "\n",
    "    characterlist=list(set(characterlist))\n",
    "    temp_=[]\n",
    "    for i,char in enumerate(characterlist):\n",
    "        persdict=dict()\n",
    "        persdict['sentenceBookID']=i\n",
    "        persdict['name']=char\n",
    "        persdict['bookID']=bookid\n",
    "        temp_.append(persdict)\n",
    "    return temp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBookFixture(dfs):\n",
    "    fixtures_=[] # 'bookindex','id', 'title','author','authoryearofbirth','authoryearofdeath','language','subjects'\n",
    "    for i,df in enumerate(dfs):\n",
    "        series=df.iloc[0]\n",
    "        fixtures_.append({'pk':series.bookindex,\n",
    "                          'id':series.id,\n",
    "                          'title':series.title,\n",
    "                          'author':series.author,\n",
    "                          'authoryearofbirth':series.authoryearofbirth,\n",
    "                          'authoryearofdeath':series.authoryearofdeath,\n",
    "                          'language':series.language,\n",
    "                          'subjects':series.subjects\n",
    "                          })\n",
    "        \n",
    "    return fixtures_\n",
    "\n",
    "def createSentFixture(df, bookFix, topicFix):\n",
    "    temp_=[]# sentenceid(pk), bookid, sentenceText,sentIDBook, topicID(pk)\n",
    "    for i,row in df.iterrows():\n",
    "        try:\n",
    "            topic=next(filter(lambda x: x['Topic']==row.Topic and x['Name']==row.Name,topicFix))\n",
    "        except  TypeError as err:\n",
    "            print(\"No Topic Found\")            \n",
    "        try:\n",
    "            book=next(filter(lambda x: x['id']==row.id,bookFix))\n",
    "        except TypeError as err:\n",
    "            print(\"Noo book found\")\n",
    "        temp_.append(\n",
    "            {\n",
    "            'pk':row.sentenceIndex,\n",
    "            'bookID': book['pk'],\n",
    "            'sentenceText':row.sents,\n",
    "            'sentIDBook':i,\n",
    "            'topicID':topic['pk'],\n",
    "            }\n",
    "        )\n",
    "    return temp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    df.rename({'index':'sentenceIndex'},axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractChar(df):\n",
    "    allChars = list()\n",
    "    for i,row in df.iterrows():\n",
    "        chars_ =row.PERS\n",
    "        for char in chars_:\n",
    "            allChars.append(char)\n",
    "    return list(set(allChars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTopics(df):\n",
    "    rel_cols=['bookindex','id','Topic','Name','Top_n_words']\n",
    "    temp_ = df[rel_cols]\n",
    "    listoftopics=list()\n",
    "    for i,row in temp_.iterrows():\n",
    "        dict_=dict()\n",
    "        dict_['Topic']=row.Topic\n",
    "        dict_['Name']=row.Name\n",
    "        dict_['Top_n_words']=processTopics(row.Top_n_words)\n",
    "        dict_['bookindex']=row.bookindex\n",
    "        dict_['id']=row.id\n",
    "        listoftopics.append(dict_)\n",
    "    return listoftopics\n",
    "\n",
    "def processTopics(topnwords):\n",
    "    list_of_words=topnwords.split('-')\n",
    "    list_of_words=[word.strip(' ') for word in list_of_words]\n",
    "    return list_of_words\n",
    "\n",
    "\n",
    "def removeDup(l):\n",
    "    new_d = []\n",
    "    for x in l:\n",
    "        if x not in new_d:\n",
    "            new_d.append(x)\n",
    "    return new_d\n",
    "\n",
    "def removeDictDuplicates(l):\n",
    "    seen = set()\n",
    "    new_l = []\n",
    "    for d in l:\n",
    "        t = tuple(d.items())\n",
    "        if t not in seen:\n",
    "            seen.add(t)\n",
    "            new_l.append(d)        \n",
    "    return sorted(new_l, key=lambda x: x['Topic'])   # sort by topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "characterTable=[]\n",
    "for df in dfs:\n",
    "    characterTable.extend(createCharacterInfo(df))\n",
    "\n",
    "characterTable =  [{'pk':i, 'name':char['name'],'bookID':char['bookID'], 'sentenceBookID':char['sentenceBookID']}for i,char in enumerate(characterTable)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentenceIndex</th>\n",
       "      <th>bookindex</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>authoryearofbirth</th>\n",
       "      <th>authoryearofdeath</th>\n",
       "      <th>language</th>\n",
       "      <th>downloads</th>\n",
       "      <th>subjects</th>\n",
       "      <th>type</th>\n",
       "      <th>sents</th>\n",
       "      <th>Document</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Name</th>\n",
       "      <th>Top_n_words</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Representative_document</th>\n",
       "      <th>PERS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17952</td>\n",
       "      <td>3</td>\n",
       "      <td>PG8446</td>\n",
       "      <td>The Enormous Room</td>\n",
       "      <td>Cummings, E. E. (Edward Estlin)</td>\n",
       "      <td>1894</td>\n",
       "      <td>1962</td>\n",
       "      <td>['en']</td>\n",
       "      <td>131</td>\n",
       "      <td>{'War stories', 'Concentration camp inmates --...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_said_time_eyes_little</td>\n",
       "      <td>said - time - eyes - little - great - say - fa...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17953</td>\n",
       "      <td>3</td>\n",
       "      <td>PG8446</td>\n",
       "      <td>The Enormous Room</td>\n",
       "      <td>Cummings, E. E. (Edward Estlin)</td>\n",
       "      <td>1894</td>\n",
       "      <td>1962</td>\n",
       "      <td>['en']</td>\n",
       "      <td>131</td>\n",
       "      <td>{'War stories', 'Concentration camp inmates --...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Illustration] The Enormous Room by E. E. Cumm...</td>\n",
       "      <td>[Illustration] The Enormous Room by E. E. Cumm...</td>\n",
       "      <td>3</td>\n",
       "      <td>3_room_enormous_pillars_inhabitants</td>\n",
       "      <td>room - enormous - pillars - inhabitants - eccl...</td>\n",
       "      <td>0.495287</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17954</td>\n",
       "      <td>3</td>\n",
       "      <td>PG8446</td>\n",
       "      <td>The Enormous Room</td>\n",
       "      <td>Cummings, E. E. (Edward Estlin)</td>\n",
       "      <td>1894</td>\n",
       "      <td>1962</td>\n",
       "      <td>['en']</td>\n",
       "      <td>131</td>\n",
       "      <td>{'War stories', 'Concentration camp inmates --...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FOUND.” He was lost by the Norton-Harjes Ambul...</td>\n",
       "      <td>FOUND.” He was lost by the Norton-Harjes Ambul...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_said_time_eyes_little</td>\n",
       "      <td>said - time - eyes - little - great - say - fa...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17955</td>\n",
       "      <td>3</td>\n",
       "      <td>PG8446</td>\n",
       "      <td>The Enormous Room</td>\n",
       "      <td>Cummings, E. E. (Edward Estlin)</td>\n",
       "      <td>1894</td>\n",
       "      <td>1962</td>\n",
       "      <td>['en']</td>\n",
       "      <td>131</td>\n",
       "      <td>{'War stories', 'Concentration camp inmates --...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>to life—with the help of powerful and willing ...</td>\n",
       "      <td>to life—with the help of powerful and willing ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_said_time_eyes_little</td>\n",
       "      <td>said - time - eyes - little - great - say - fa...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17956</td>\n",
       "      <td>3</td>\n",
       "      <td>PG8446</td>\n",
       "      <td>The Enormous Room</td>\n",
       "      <td>Cummings, E. E. (Edward Estlin)</td>\n",
       "      <td>1894</td>\n",
       "      <td>1962</td>\n",
       "      <td>['en']</td>\n",
       "      <td>131</td>\n",
       "      <td>{'War stories', 'Concentration camp inmates --...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the Atlantic. The following documents tell the...</td>\n",
       "      <td>the Atlantic. The following documents tell the...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_said_time_eyes_little</td>\n",
       "      <td>said - time - eyes - little - great - say - fa...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentenceIndex  bookindex      id              title  \\\n",
       "0          17952          3  PG8446  The Enormous Room   \n",
       "1          17953          3  PG8446  The Enormous Room   \n",
       "2          17954          3  PG8446  The Enormous Room   \n",
       "3          17955          3  PG8446  The Enormous Room   \n",
       "4          17956          3  PG8446  The Enormous Room   \n",
       "\n",
       "                            author  authoryearofbirth  authoryearofdeath  \\\n",
       "0  Cummings, E. E. (Edward Estlin)               1894               1962   \n",
       "1  Cummings, E. E. (Edward Estlin)               1894               1962   \n",
       "2  Cummings, E. E. (Edward Estlin)               1894               1962   \n",
       "3  Cummings, E. E. (Edward Estlin)               1894               1962   \n",
       "4  Cummings, E. E. (Edward Estlin)               1894               1962   \n",
       "\n",
       "  language  downloads                                           subjects  \\\n",
       "0   ['en']        131  {'War stories', 'Concentration camp inmates --...   \n",
       "1   ['en']        131  {'War stories', 'Concentration camp inmates --...   \n",
       "2   ['en']        131  {'War stories', 'Concentration camp inmates --...   \n",
       "3   ['en']        131  {'War stories', 'Concentration camp inmates --...   \n",
       "4   ['en']        131  {'War stories', 'Concentration camp inmates --...   \n",
       "\n",
       "   type                                              sents  \\\n",
       "0   NaN                                                      \n",
       "1   NaN  [Illustration] The Enormous Room by E. E. Cumm...   \n",
       "2   NaN  FOUND.” He was lost by the Norton-Harjes Ambul...   \n",
       "3   NaN  to life—with the help of powerful and willing ...   \n",
       "4   NaN  the Atlantic. The following documents tell the...   \n",
       "\n",
       "                                            Document  Topic  \\\n",
       "0                                                        -1   \n",
       "1  [Illustration] The Enormous Room by E. E. Cumm...      3   \n",
       "2  FOUND.” He was lost by the Norton-Harjes Ambul...     -1   \n",
       "3  to life—with the help of powerful and willing ...     -1   \n",
       "4  the Atlantic. The following documents tell the...     -1   \n",
       "\n",
       "                                  Name  \\\n",
       "0             -1_said_time_eyes_little   \n",
       "1  3_room_enormous_pillars_inhabitants   \n",
       "2             -1_said_time_eyes_little   \n",
       "3             -1_said_time_eyes_little   \n",
       "4             -1_said_time_eyes_little   \n",
       "\n",
       "                                         Top_n_words  Probability  \\\n",
       "0  said - time - eyes - little - great - say - fa...     0.000000   \n",
       "1  room - enormous - pillars - inhabitants - eccl...     0.495287   \n",
       "2  said - time - eyes - little - great - say - fa...     0.000000   \n",
       "3  said - time - eyes - little - great - say - fa...     0.000000   \n",
       "4  said - time - eyes - little - great - say - fa...     0.000000   \n",
       "\n",
       "   Representative_document PERS  \n",
       "0                    False   []  \n",
       "1                    False   []  \n",
       "2                    False   []  \n",
       "3                    False   []  \n",
       "4                    False   []  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=removeDup(extractTopics(df))\n",
    "t =  [{'pk':i, 'Topic':topic['Topic'],'Name':topic['Name'], 'top_n_word':topic['Top_n_words'],'bookindex':topic['bookindex']}for i,topic in enumerate(t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSent2Char(df, dfs_character):\n",
    "\n",
    "    def helpFilter(charDict, char):\n",
    "        return charDict['name']==char\n",
    "    \n",
    "    sent2char=[]\n",
    "\n",
    "    for i,row in df.iterrows():\n",
    "        characters=row.PERS\n",
    "        for char in characters:\n",
    "            filteredChar = next(filter(lambda x: helpFilter(x,char), dfs_character))\n",
    "            charpk = filteredChar['pk']\n",
    "            sentID = row.sentenceIndex\n",
    "            sent2char.append({'charID':charpk, 'sentID':sentID})\n",
    "\n",
    "    return sent2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isTopic(id,item):\n",
    "    \"\"\"\n",
    "    Filter Function for filter()\n",
    "    \n",
    "    \"\"\"\n",
    "    return id==item['Topic']   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_book=[] # 'bookindex','id', 'title','author','authoryearofbirth','authoryearofdeath','language','subjects'\n",
    "dfs_sentence=[] # sentenceid(pk), bookid, sentenceText,sentIDBook, topicID(pk)\n",
    "dfs_Sent2Char=[] # charID(sent.pk), sentID(sent.pk)\n",
    "dfs_character=[] # id(pk),name, bookid(book(pk))\n",
    "dfs_topic=[] # id(pk), bookid(book.pk), bookInternID(topicID),name, topnwords\n",
    "for df in dfs:\n",
    "    #df=df.reset_index().rename({'index':'sentIDBook'},axis=1)\n",
    "    dfs_book.append(df[['bookindex','id', 'title','author','authoryearofbirth','authoryearofdeath','language','subjects']])\n",
    "    dfs_sentence.append(df[['sentenceIndex', 'id','sents','Topic']])\n",
    "    dfs_character.extend(createCharacterInfo(df))\n",
    "    dfs_topic.extend(removeDup(extractTopics(df)))\n",
    "\n",
    "characterFixture =  [{'pk':i, 'name':char['name'],'bookID':char['bookID'], 'sentenceBookID':char['sentenceBookID']}for i,char in enumerate(dfs_character)]\n",
    "topicFixture =  [{'pk':i, 'Topic':topic['Topic'],'Name':topic['Name'], 'Top_n_words':topic['Top_n_words'],'bookindex':topic['bookindex']}for i,topic in enumerate(dfs_topic)]\n",
    "bookFixture=createBookFixture(dfs)\n",
    "sentenceFixture=[]\n",
    "sent2charFixture=[]\n",
    "for df in dfs:\n",
    "    sent2charFixture.extend(createSent2Char(df, characterFixture))\n",
    "    fitures=createSentFixture(df,bookFixture,topicFixture)\n",
    "    sentenceFixture.extend(fitures)\n",
    "\n",
    "sent2charFixture=[{'pk':i, 'charID':fix['charID'], 'sentID':fix['sentID']} for i,fix in enumerate(sent2charFixture)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'fields'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m list_of_Sent\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39;49m(\u001b[39mfilter\u001b[39;49m(\u001b[39mlambda\u001b[39;49;00m x: x[\u001b[39m'\u001b[39;49m\u001b[39mfields\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mcharID\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m==\u001b[39;49m\u001b[39m15\u001b[39;49m, sent2charFixture))\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m list_of_Sent:\n\u001b[1;32m      4\u001b[0m     toSearch\u001b[39m=\u001b[39msent[\u001b[39m'\u001b[39m\u001b[39mfields\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39msentID\u001b[39m\u001b[39m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m list_of_Sent\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m(\u001b[39mfilter\u001b[39m(\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m'\u001b[39;49m\u001b[39mfields\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39m\u001b[39mcharID\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m==\u001b[39m\u001b[39m15\u001b[39m, sent2charFixture))\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m list_of_Sent:\n\u001b[1;32m      4\u001b[0m     toSearch\u001b[39m=\u001b[39msent[\u001b[39m'\u001b[39m\u001b[39mfields\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39msentID\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'fields'"
     ]
    }
   ],
   "source": [
    "list_of_Sent=list(filter(lambda x: x['fields']['charID']==15, sent2charFixture))\n",
    "\n",
    "for sent in list_of_Sent:\n",
    "    toSearch=sent['fields']['sentID']\n",
    "    sentences=list(filter(lambda x: x['pk']==toSearch,sentenceFixture))\n",
    "\n",
    "bookid=next(filter(lambda x: x['pk']==15, characterFixture))['fields']['bookID']\n",
    "allTopics=list(filter(lambda x: x['fields']['bookID']==bookid, topicFixture))\n",
    "\n",
    "distr_=dict()\n",
    "for topic in allTopics:\n",
    "    distr_[topic['fields']['TopicID']]= {'TopicName:':topic['fields']['TopicName'], 'Count':0}\n",
    "for sent in sentences:\n",
    "    distr_[sent['fields']['topicID']]['Count']+=1\n",
    "\n",
    "\n",
    "print(distr_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFinalCharacterFixtur(fixture):\n",
    "    temp_=[]\n",
    "    for fix in fixture:\n",
    "        temp_.append(\n",
    "            {\n",
    "            \"model\":\"webDB.Character\",\n",
    "            \"pk\":fix['pk'],\n",
    "            \"fields\": {\n",
    "                'Name':fix['name'],\n",
    "                'bookID':fix['bookID'],\n",
    "            }\n",
    "            }\n",
    "        )\n",
    "    return temp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFinalTopicFixtur(fixture):\n",
    "    temp_=[]\n",
    "    for fix in fixture:\n",
    "        temp_.append(\n",
    "            {\n",
    "            \"model\":\"webDB.Topic\",\n",
    "            \"pk\":fix['pk'],\n",
    "            \"fields\": {\n",
    "                'TopicID':fix['Topic'],\n",
    "                'TopicName':fix['Name'],\n",
    "                'Top_n_words':fix['Top_n_words'],\n",
    "                'bookID':fix['bookindex']\n",
    "            }\n",
    "            }\n",
    "        )\n",
    "    return temp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFinalSent2CharFixtur(fixture):\n",
    "    temp_=[]\n",
    "    for fix in fixture:\n",
    "        temp_.append(\n",
    "            {\n",
    "            \"model\":\"webDB.Sent2Char\",\n",
    "            \"pk\":fix['pk'],\n",
    "            \"fields\": {\n",
    "                'charID':fix['charID'],\n",
    "                'sentID':fix['sentID']\n",
    "            }\n",
    "            }\n",
    "        )\n",
    "    return temp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFinalSentenceFixtur(fixture):\n",
    "    temp_=[]\n",
    "    for fix in fixture:\n",
    "        temp_.append(\n",
    "            {\n",
    "            \"model\":\"webDB.Sentence\",\n",
    "            \"pk\":int(fix['pk']),\n",
    "            \"fields\": {\n",
    "                'bookID':int(fix['bookID']),\n",
    "                'sentenceText':fix['sentenceText'],\n",
    "                'sentIDBook':fix['sentIDBook'],\n",
    "                'topicID':fix['topicID']\n",
    "            }\n",
    "            }\n",
    "        )\n",
    "    return temp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFinalBookFixtur(fixture):\n",
    "    temp_=[]\n",
    "    for fix in fixture:\n",
    "        temp_.append(\n",
    "            {\n",
    "            \"model\":\"webDB.Book\",\n",
    "            \"pk\":int(fix['pk']),\n",
    "            \"fields\": {\n",
    "                \"bookID\":fix['id'],\n",
    "                \"bookName\":fix['title'],\n",
    "                \"bookauthor\":fix['author'],\n",
    "                \"bookAuthorYearOfBirth\":str(fix['authoryearofbirth']),\n",
    "                \"bookAuthorYearOfDeath\":str(fix['authoryearofdeath']),\n",
    "                \"bookLanguage\":fix['language'],\n",
    "                \"bookSubjects\":fix['subjects'],\n",
    "            }\n",
    "            }\n",
    "        )\n",
    "    return temp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "characterFixture=createFinalCharacterFixtur(characterFixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicFixture=createFinalTopicFixtur(topicFixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookFixture=createFinalBookFixtur(bookFixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceFixture=createFinalSentenceFixtur(sentenceFixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2charFixture=createFinalSent2CharFixtur(sent2charFixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_int(x):\n",
    "    x = x['fields']['bookID']\n",
    "    print(x)\n",
    "    return int(x)\n",
    "sentenceFixture=[to_int(sent)for sent in sentenceFixture]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "fixtureFileNames=['character','topic','book','sentence','sent2char']\n",
    "fixturesCombined=[characterFixture,topicFixture,bookFixture,sentenceFixture,sent2charFixture]\n",
    "for fixture, filename in zip(fixturesCombined,fixtureFileNames):\n",
    "    with open(f\"./fixtures/Fixtures_Mid/{filename}FixturesSmall.json\", 'w',encoding='utf-8') as f:\n",
    "        json.dump(fixture,f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gutenbergDBInit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
